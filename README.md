# ArtiHub

- Arda Selim Arslan - 22303115
- Bilgehan Akın - 22203657
- Deniz Can Yoldaş - 22303392
- Emre Altuner - 22303937
- Rıfat Arifoğlu - 22303017

## Project Description

Our project proposal is to design and implement a web-based application called the Artifact Comparator, which will enable researchers to conduct experiments that involve evaluating and analyzing various software engineering artifacts. This platform is motivated by the need for a structured method of conducting human-subject evaluations in software engineering research. In modern development environments, artifacts such as source code, UML diagrams, test cases, and documentation are created both by humans and increasingly by automated tools such as Large Language Models (LLMs). Evaluating the quality and correctness of these artifacts is often time consuming. Our system aims to address this issue by providing a platform that supports reproducible and structured evaluations with rich annotation and feedback mechanisms.

The main goals of the project are to provide researchers with a platform to design and run studies involving software artifact evaluation, enable participants to compare and annotate multiple artifact types side by side, facilitate data collection and analysis for research purposes and ensure a secure, extensible, and user-friendly environment. Ultimately, the system will serve as a bridge between automated artifact generation and human evaluation, creating opportunities to better understand the effectiveness of human-created versus AI-generated software artifacts.

The problem our web app solves is the lack of dedicated tools for conducting controlled artifact evaluations in software engineering. Currently, researchers rely on solutions such as forms, surveys or manual comparison methods, which limit scalability and consistency. By offering an integrated environment, our system reduces researcher overhead while improving the quality and reliability of evaluation data.

The key features of the web app will include artifact upload and management, participant background and competency assessment, side-by-side artifact comparison with annotation and rating tools, blinded evaluation modes, dashboards for researchers and participants, and exportable reports for analysis. Additionally, the system will support integration with external artifact analysis tools, providing metrics such as code complexity or style reports alongside user evaluations. Both local and hosted deployment modes will be available, ensuring flexibility in usage.The system will include several user roles with specific permissions and responsibilities. Researchers will design and manage studies, upload and tag artifacts, define evaluation criteria, and analyze participant feedback through dashboards; however, they cannot access other researchers’ private studies. Participants will complete competency assessments and perform artifact comparisons through an intuitive interface but will have access only to their assigned tasks and anonymized artifacts. Reviewers will be able to audit evaluation quality and ensure data consistency without modifying study content or participant data. Finally, Admins will oversee user management, system configuration, and security but will not interfere with the research content or participant evaluations.

The selling points of the app lie in its flexibility, structured study orchestration, and extensibility. Unlike generic survey platforms, our system is tailored specifically for software artifact comparison and is capable of handling heterogeneous inputs such as source code, test cases, and documentation. With its researcher dashboard that enables to track the participants and ongoing studies, it simplifies to orchestrate the study for the researcher. By the artifact evaluation features(annotation, side by side comparison etc.), it also simplifies the process for participants. Since the application is open to extensions like new artifact types, it has an ability to adapt changes in the future. In general, since it increases effectiveness for both sides and adaptability the application has a potential to be sold to others.

What makes this web app interesting is its modern AI based features. Participant competency assessment quizzes and artifacts can be generated by AI. Since AI is a trending topic in computer science, AI based features make the application interesting and cool.
	
## Extra Features

- ### Integrated Forum with Moderation
> A dedicated forum will be available where users can discuss artifacts, share insights, and ask questions. A new Moderator role will be introduced to manage this space, with powers such as banning, muting, or removing disruptive participants to maintain a constructive environment.

- ### Competency-Based Participant Levels
> Participants will be grouped into different levels based on their competency. This helps researchers target evaluations more effectively and ensures fair comparisons between participants of similar skill levels.

- ### GitHub OAuth Login
> To simplify the signing in process and improve security, the system will support authentication via GitHub OAuth. This allows participants and researchers to log in quickly using their existing GitHub accounts, reducing the need for separate credentials.

- ### Automated Citation Tools
> The platform will include built-in tools to generate citations for studies hosted on the website. This feature makes it easier for researchers to reference their work in academic papers and ensures consistent citation formatting.

- ### Study Invitation via URL Links
> Researchers will be able to generate secure invitation links for their studies. This allows them to recruit participants easily by sharing a URL, removing the need for manual registration processes.
